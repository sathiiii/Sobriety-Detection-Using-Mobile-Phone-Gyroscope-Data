{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is used to implement the python scripts for segmenting the data using windowing techinque with 50% overlap based on the number of samples (data is segmented into equal sized windows)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUyNFFEHsUe6"
      },
      "source": [
        "## Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "yD7y6Ge8rj8x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy import fftpack, signal\n",
        "from scipy.fftpack.realtransforms import dct\n",
        "from scipy.stats import skew, kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFU3vpYttgwm",
        "outputId": "7e44f454-29fd-4940-9c5a-c5b249daccee"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '../Datasets/Accelerometer Data + TAC'\n",
        "ACCELEROMETER_DATA = 'all_accelerometer_data_pids_13.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each axis, we will use a set of statistical metrics as well as some spectral features per window of 10 seconds in length. The window size is chosen arbitrarily and has to be tuned for the optimal value. The idea is to subdivide a window further into smaller windows of 1 second each and compute the metrics for each short-term window (this is also known as the two-tiered window approach) to characterize the data as it changes with time. Then, we'll compute the mean, variance, min and max along with the mean of the lower third and upper third of sorted values, creating a total of 6 summary statistics per metric per 10 second window per axis that results in 18 features per metric in total. We'll also calculate the difference between the previous window and the current window for each feature which will double the number of features per window per metric (36).\n",
        "\n",
        "The feature extraction is done per participant data with the intention of splitting the data into training and test sets by participant. To keep the most relevant features, we will calculate the feature importance of each feature by converting the feature selection problem into a supervised learning problem. The traditional way of doing feature selection is computing the correlation between the features and removing one of highly correlated pairs of features and keeping the other. However, when there're many features, this approach is not feasible. Hence, we will use a Random Forest regressor to compute the feature importance. To select features automatically, we will use Recursive Feature Elimination (RFE) that'll use the random forest regressor as the predictive model to weight features and prune those with the smallest weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def zero_crossing_rate(data):\n",
        "    return np.sum(np.abs(np.diff(np.sign(data)))) / (2 * (len(data) - 1))\n",
        "\n",
        "def spectral_entropy(signal, n_short_blocks=10, eps=1e-9):\n",
        "    \"\"\"Computes the spectral entropy\"\"\"\n",
        "    # number of frame samples\n",
        "    num_frames = len(signal)\n",
        "    # total spectral energy\n",
        "    total_energy = np.sum(signal ** 2)\n",
        "    # length of sub-frame\n",
        "    sub_win_len = int(np.floor(num_frames / n_short_blocks))\n",
        "    if num_frames != sub_win_len * n_short_blocks:\n",
        "        signal = signal[0:sub_win_len * n_short_blocks]\n",
        "    # define sub-frames (using matrix reshape)\n",
        "    sub_wins = signal.reshape(sub_win_len, n_short_blocks, order='F').copy()\n",
        "    # compute spectral sub-energies\n",
        "    s = np.sum(sub_wins ** 2, axis=0) / (total_energy + eps)\n",
        "    # compute spectral entropy\n",
        "    entropy = -np.sum(s * np.log2(s + eps))\n",
        "    return entropy\n",
        "\n",
        "def spectral_centroid(fft_magnitude, sampling_rate=40, eps=1e-9):\n",
        "    ind = (np.arange(1, len(fft_magnitude) + 1)) * (sampling_rate / (2.0 * len(fft_magnitude)))\n",
        "    Xt = fft_magnitude.copy()\n",
        "    Xt = Xt / Xt.max()\n",
        "    NUM = np.sum(ind * Xt)\n",
        "    DEN = np.sum(Xt) + eps\n",
        "    # Centroid:\n",
        "    centroid = (NUM / DEN)\n",
        "    # Normalize:\n",
        "    centroid = centroid / (sampling_rate / 2.0)\n",
        "    return centroid\n",
        "\n",
        "def spectral_spread(fft_magnitude, sampling_rate=40, eps=1e-9):\n",
        "    ind = (np.arange(1, len(fft_magnitude) + 1)) * (sampling_rate / (2.0 * len(fft_magnitude)))\n",
        "    Xt = fft_magnitude.copy()\n",
        "    Xt = Xt / Xt.max()\n",
        "    NUM = np.sum(ind * Xt)\n",
        "    DEN = np.sum(Xt) + eps\n",
        "    # Spread:\n",
        "    spread = np.sqrt(np.sum(((ind - (NUM / DEN)) ** 2) * Xt) / DEN)\n",
        "    # Normalize:\n",
        "    spread = spread / (sampling_rate / 2.0)\n",
        "    return spread\n",
        "\n",
        "def spectral_flux(fft_magnitude, previous_fft_magnitude, eps=1e-9):\n",
        "    # compute the spectral flux as the sum of square distances:\n",
        "    fft_sum = np.sum(fft_magnitude + eps)\n",
        "    previous_fft_sum = np.sum(previous_fft_magnitude + eps)\n",
        "    sp_flux = np.sum((fft_magnitude / fft_sum - previous_fft_magnitude / previous_fft_sum) ** 2)\n",
        "    return sp_flux\n",
        "\n",
        "def spectral_rolloff(signal, c=0.90, eps=1e-9):\n",
        "    energy = np.sum(signal ** 2)\n",
        "    fft_length = len(signal)\n",
        "    threshold = c * energy\n",
        "    # Find the spectral rolloff as the frequency position where the respective spectral energy is equal to c*totalEnergy\n",
        "    cumulative_sum = np.cumsum(signal ** 2) + eps\n",
        "    a = np.nonzero(cumulative_sum > threshold)[0]\n",
        "    sp_rolloff = 0.0\n",
        "    if len(a) > 0: sp_rolloff = np.float64(a[0]) / (float(fft_length))\n",
        "    return sp_rolloff\n",
        "\n",
        "def spectral_peak_ratio(fft_magnitude, eps=1e-9):\n",
        "    # Ratio of largest peak to second largest peak\n",
        "    peaks = sorted(fft_magnitude, reverse=True)\n",
        "    if len(peaks) < 2: return 0.0\n",
        "    return peaks[0] / (peaks[1] + eps)\n",
        "\n",
        "def max_frequency(fft_magnitude, sampling_rate=40):\n",
        "    max_freq = np.argmax(fft_magnitude)\n",
        "    max_freq *= (sampling_rate / (2.0 * len(fft_magnitude)))\n",
        "    return max_freq\n",
        "\n",
        "def mfcc_filter_banks(sampling_rate, num_fft, lowfreq=133.33, linc=200 / 3,\n",
        "                      logsc=1.0711703, num_lin_filt=13, num_log_filt=27):\n",
        "    \"\"\"\n",
        "    Computes the triangular filterbank for MFCC computation \n",
        "    (used in the stFeatureExtraction function before the stMFCC function call)\n",
        "    This function is taken from the scikits.talkbox library (MIT Licence):\n",
        "    https://pypi.python.org/pypi/scikits.talkbox\n",
        "    \"\"\"\n",
        "\n",
        "    if sampling_rate < 8000:\n",
        "        nlogfil = 5\n",
        "\n",
        "    # Total number of filters\n",
        "    num_filt_total = num_lin_filt + num_log_filt\n",
        "\n",
        "    # Compute frequency points of the triangle:\n",
        "    frequencies = np.zeros(num_filt_total + 2)\n",
        "    frequencies[:num_lin_filt] = lowfreq + np.arange(num_lin_filt) * linc\n",
        "    frequencies[num_lin_filt:] = frequencies[num_lin_filt - 1] * logsc ** \\\n",
        "                                 np.arange(1, num_log_filt + 3)\n",
        "    heights = 2. / (frequencies[2:] - frequencies[0:-2])\n",
        "\n",
        "    # Compute filterbank coeff (in fft domain, in bins)\n",
        "    fbank = np.zeros((num_filt_total, num_fft))\n",
        "    nfreqs = np.arange(num_fft) / (1. * num_fft) * sampling_rate\n",
        "\n",
        "    for i in range(num_filt_total):\n",
        "        low_freqs = frequencies[i]\n",
        "        cent_freqs = frequencies[i + 1]\n",
        "        high_freqs = frequencies[i + 2]\n",
        "\n",
        "        lid = np.arange(np.floor(low_freqs * num_fft / sampling_rate) + 1,\n",
        "                        np.floor(cent_freqs * num_fft / sampling_rate) + 1,\n",
        "                        dtype=np.int32)\n",
        "        lslope = heights[i] / (cent_freqs - low_freqs)\n",
        "        rid = np.arange(np.floor(cent_freqs * num_fft / sampling_rate) + 1,\n",
        "                        np.floor(high_freqs * num_fft / sampling_rate) + 1,\n",
        "                        dtype=np.int32)\n",
        "        rslope = heights[i] / (high_freqs - cent_freqs)\n",
        "        print(lid)\n",
        "        fbank[i][lid] = lslope * (nfreqs[lid] - low_freqs)\n",
        "        fbank[i][rid] = rslope * (high_freqs - nfreqs[rid])\n",
        "\n",
        "    return fbank, frequencies\n",
        "\n",
        "def mfcc(fft_magnitude, fbank, num_mfcc_feats, eps=1e-9):\n",
        "    \"\"\"\n",
        "    Computes the MFCCs of a frame, given the fft mag\n",
        "    ARGUMENTS:\n",
        "        fft_magnitude:  fft magnitude abs(FFT)\n",
        "        fbank:          filter bank (see mfccInitFilterBanks)\n",
        "    RETURN\n",
        "        ceps:           MFCCs (13 element vector)\n",
        "    Note:    MFCC calculation is, in general, taken from the \n",
        "             scikits.talkbox library (MIT Licence),\n",
        "    #    with a small number of modifications to make it more \n",
        "         compact and suitable for the pyAudioAnalysis Lib\n",
        "    \"\"\"\n",
        "\n",
        "    mspec = np.log10(np.dot(fft_magnitude, fbank.T) + eps)\n",
        "    ceps = dct(mspec, type=2, norm='ortho', axis=-1)[:num_mfcc_feats]\n",
        "    return ceps\n",
        "\n",
        "def avg_power(sig):\n",
        "    _, power = signal.welch(sig, 40, nperseg=len(sig))\n",
        "    return np.mean(power)\n",
        "\n",
        "def rms(signal):\n",
        "    return np.sqrt(np.mean(signal ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(f'{DATASET_PATH}/feature_data_new'):\n",
        "    os.makedirs(f'{DATASET_PATH}/feature_data_new')\n",
        "else:\n",
        "    for file in os.listdir(f'{DATASET_PATH}/feature_data_new'):\n",
        "        os.remove(f'{DATASET_PATH}/feature_data_new/{file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'mean': np.mean, \n",
        "    'std': np.std,\n",
        "    'avg_abs_dev': lambda x: np.mean(np.abs(x - np.mean(x))),\n",
        "    'min_raw': np.min,\n",
        "    'max_raw': np.max,\n",
        "    'min_abs': lambda x: np.min(np.abs(x)),\n",
        "    'max_abs': lambda x: np.max(np.abs(x)), \n",
        "    'median': np.median, \n",
        "    'inter_quartile_range': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
        "    'zero_crossing_rate': zero_crossing_rate, \n",
        "    'skewness': skew, \n",
        "    'kurtosis': kurtosis, \n",
        "    'spectral_entropy': spectral_entropy, \n",
        "    'fft_spectral_entropy': spectral_entropy,\n",
        "    'fft_spectral_centroid': spectral_centroid, \n",
        "    'fft_spectral_spread': spectral_spread,\n",
        "    'fft_spectral_rolloff': spectral_rolloff,\n",
        "    'fft_spectral_peak_ratio': spectral_peak_ratio,\n",
        "    'avg_power': avg_power,\n",
        "    'rms': rms,\n",
        "    'max_freq': max_frequency,\n",
        "    'fft_spectral_flux': spectral_flux,\n",
        "    'mfcc': None # Mel Frequency Cepstral Coefficients (MFCC)\n",
        "}\n",
        "\n",
        "n_mfcc_features = 13\n",
        "sampling_rate = 40\n",
        "long_seg_len, short_seg_len = 400, 40\n",
        "\n",
        "summary_stats = {\n",
        "    'mean': np.mean,\n",
        "    'variance': np.var,\n",
        "    'min': np.min,\n",
        "    'max': np.max,\n",
        "    'lower_third_mean': lambda x: np.mean(sorted(x)[:len(x) // 3]),\n",
        "    'upper_third_mean': lambda x: np.mean(sorted(x)[len(x) // 3:])\n",
        "}\n",
        "\n",
        "# Compute the triangular filter banks used in the mfcc calculation\n",
        "# fbank, freqs = mfcc_filter_banks(sampling_rate, short_seg_len // 2)\n",
        "\n",
        "for acc_data in os.listdir(f'{DATASET_PATH}/clean_participant_data'):\n",
        "    pid = acc_data.split('_')[0]\n",
        "    data = pd.read_csv(f'{DATASET_PATH}/clean_participant_data/{acc_data}')\n",
        "    feature_df = pd.DataFrame()\n",
        "    columns = []\n",
        "    ground_truth = []\n",
        "    start, long_seg_id = 0, 0\n",
        "\n",
        "    # Assume uniform frequency over the data samples\n",
        "    # A long segment is of 10 seconds (400 samples) and a short segment is of 1 second (40 samples)\n",
        "    # Long windows are segmented with 50% overlap\n",
        "    # Short segments are sub-segments of the long segments in order to calculate features from a two-tiered approach\n",
        "    for end in tqdm(range(long_seg_len, len(data), short_seg_len // 2), desc=f'Processing {pid}'):\n",
        "        # Split the current long segment into 10 short segments\n",
        "        short_segs = np.array_split(data.loc[start:end - 1, ['x', 'y', 'z']].to_numpy(), long_seg_len // short_seg_len)\n",
        "        # Compute the fft magnitude spectrum for each short segment\n",
        "        short_seg_fft_mag = [np.apply_along_axis(fftpack.fft, 1, short_seg) for short_seg in short_segs]\n",
        "        short_seg_fft_mag = [np.abs(fft_mag) for fft_mag in short_seg_fft_mag]\n",
        "        # Find the ground truth label for the current long segment (the most frequent label in the segment)\n",
        "        ground_truth.append(data.loc[start:end - 1, 'ground_truth'].value_counts().idxmax())\n",
        "        # Store the feature values for the current long segment\n",
        "        row = []\n",
        "\n",
        "        # Apply each metric for each short segment for each axis readings of the current long segment\n",
        "        for metric_name, metric in metrics.items():\n",
        "            short_term_features = {'x': [], 'y': [], 'z': []}\n",
        "            mfcc_coeffs = {'x': [], 'y': [], 'z': []}\n",
        "\n",
        "            def apply_metric(seg_id, axis):\n",
        "                if metric_name.split('_')[0] != 'fft':\n",
        "                    return metric(short_segs[seg_id][:, axis])\n",
        "                if metric_name == 'fft_spectral_flux':\n",
        "                    fft_mag_prev = short_seg_fft_mag[seg_id - 1][:, axis] if seg_id > 0 else short_seg_fft_mag[seg_id][:, axis]\n",
        "                    return metric(short_seg_fft_mag[seg_id][:, axis], fft_mag_prev)\n",
        "                return metric(short_seg_fft_mag[seg_id][:, axis])\n",
        "\n",
        "            for short_seg_id in range(10):\n",
        "                if metric_name != 'mfcc':\n",
        "                    short_term_features['x'].append(apply_metric(short_seg_id, 0))\n",
        "                    short_term_features['y'].append(apply_metric(short_seg_id, 1))\n",
        "                    short_term_features['z'].append(apply_metric(short_seg_id, 2))\n",
        "                else:\n",
        "                # Compute the mfcc coefficients for the current short segment\n",
        "                    mfcc_coeffs['x'].append(librosa.feature.mfcc(y=short_seg_fft_mag[short_seg_id][:, 0], sr=40, n_mfcc=n_mfcc_features, n_fft=short_seg_len // 2, n_mels=13))\n",
        "                    mfcc_coeffs['y'].append(librosa.feature.mfcc(y=short_seg_fft_mag[short_seg_id][:, 1], sr=40, n_mfcc=n_mfcc_features, n_fft=short_seg_len // 2, n_mels=13))\n",
        "                    mfcc_coeffs['z'].append(librosa.feature.mfcc(y=short_seg_fft_mag[short_seg_id][:, 2], sr=40, n_mfcc=n_mfcc_features, n_fft=short_seg_len // 2, n_mels=13))\n",
        "\n",
        "            if metric_name == 'mfcc':\n",
        "                mfcc_coeffs['x'] = np.array(mfcc_coeffs['x']).reshape(n_mfcc_features, 10)\n",
        "                mfcc_coeffs['y'] = np.array(mfcc_coeffs['y']).reshape(n_mfcc_features, 10)\n",
        "                mfcc_coeffs['z'] = np.array(mfcc_coeffs['z']).reshape(n_mfcc_features, 10)\n",
        "                mfcc_coeffs['xx'] = mfcc_coeffs['x'] @ mfcc_coeffs['x'].T\n",
        "                mfcc_coeffs['yy'] = mfcc_coeffs['y'] @ mfcc_coeffs['y'].T\n",
        "                mfcc_coeffs['zz'] = mfcc_coeffs['z'] @ mfcc_coeffs['z'].T\n",
        "                mfcc_coeffs['xy'] = mfcc_coeffs['x'] @ mfcc_coeffs['y'].T\n",
        "                mfcc_coeffs['xz'] = mfcc_coeffs['x'] @ mfcc_coeffs['z'].T\n",
        "                mfcc_coeffs['yz'] = mfcc_coeffs['y'] @ mfcc_coeffs['z'].T\n",
        "                short_term_features['xx'] = mfcc_coeffs['xx'][np.triu_indices(n_mfcc_features)]\n",
        "                short_term_features['yy'] = mfcc_coeffs['yy'][np.triu_indices(n_mfcc_features)]\n",
        "                short_term_features['zz'] = mfcc_coeffs['zz'][np.triu_indices(n_mfcc_features)]\n",
        "                short_term_features['xy'] = mfcc_coeffs['xy'][np.triu_indices(n_mfcc_features)]\n",
        "                short_term_features['xz'] = mfcc_coeffs['xz'][np.triu_indices(n_mfcc_features)]\n",
        "                short_term_features['yz'] = mfcc_coeffs['yz'][np.triu_indices(n_mfcc_features)]\n",
        "\n",
        "            # Now compute the summary statistics for the metric for the current long segment\n",
        "            for axis, axis_data in short_term_features.items():\n",
        "                if len(axis_data) == 0: continue\n",
        "                for summary_stat_name, summary_stat in summary_stats.items():\n",
        "                    row.append(summary_stat(axis_data))\n",
        "                    if long_seg_id == 0:\n",
        "                        columns.append(f'{axis}_{metric_name}_{summary_stat_name}')\n",
        "\n",
        "        temp = row\n",
        "        if long_seg_id == 0:\n",
        "            columns.extend([f'{feature_name}_diff' for feature_name in columns])\n",
        "            feature_df = pd.DataFrame(columns=columns)\n",
        "            row *= 2\n",
        "        else:\n",
        "            row.extend([row[i] - prev_row[i] for i in range(len(row))])\n",
        "        feature_df.loc[long_seg_id] = row\n",
        "        prev_row = temp\n",
        "        start += short_seg_len // 2\n",
        "        long_seg_id += 1\n",
        "\n",
        "    feature_df['ground_truth'] = ground_truth\n",
        "    feature_df.to_csv(f'{DATASET_PATH}/feature_data_new/{pid}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Sobriety Detection using Mobile Phone Gyroscope Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
